{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d31dbb-4790-47d3-a7a2-4165e84db7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.axes import Axes\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsbombpy import sb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff5bb6-2fe4-4189-a307-a9ff5d61c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive data for 1. Bundesliga 2015/2016 from StatsBomb\n",
    "COMPETITION_ID = 9\n",
    "SEASON_ID = 27\n",
    "bundesliga_matches = sb.matches(competition_id=COMPETITION_ID, season_id=SEASON_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df96b91-1207-4329-bd1f-48cec6f6e803",
   "metadata": {},
   "source": [
    "### Explorative Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e692b4-5181-438b-86b4-6e3a2602d2b8",
   "metadata": {},
   "source": [
    "The goal of this block is to get accustomed with data, do some preprocessing/visualisation for further Elo calculation and experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad1434-3ac7-4393-a8f3-944607f8caca",
   "metadata": {},
   "source": [
    "#### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218feb35-e137-4a48-b64b-ccc8fae4eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for NAs\n",
    "bundesliga_matches.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5d5b9-9dc4-4c91-ab5c-0ddd8738c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_value_counts(df: DataFrame, column_name: str) -> List[int]:\n",
    "    \"\"\"Creates a list of unique counts of values in a specified column of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df: DataFrame containing the data.\n",
    "    column_name: A name of a column to analyze.\n",
    "\n",
    "    Returns:\n",
    "       A list of unique counts of values in the specified column.\n",
    "\n",
    "    \"\"\"\n",
    "    return list(df[column_name].value_counts().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c1e30-3555-4606-9fbb-c73f47156b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic sanity checks:\n",
    "# 1. All match weeks should have the same number of matches played\n",
    "# 2. All teams should the same number of matches played at home\n",
    "# 3. The team should play the same number of matches both at home and away\n",
    "# 4. The number of matches played at home (or either away) should be equal 2*(the number of matches played at a match week) - 1\n",
    "assert len(unique_value_counts(bundesliga_matches, \"match_week\")) == 1\n",
    "assert len(unique_value_counts(bundesliga_matches, \"home_team\")) == 1\n",
    "assert unique_value_counts(bundesliga_matches, \"home_team\") == unique_value_counts(\n",
    "    bundesliga_matches, \"away_team\"\n",
    ")\n",
    "assert (\n",
    "    unique_value_counts(bundesliga_matches, \"home_team\")[0]\n",
    "    == 2 * (unique_value_counts(bundesliga_matches, \"match_week\")[0]) - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cf891-f84e-458a-bed2-254f05afb937",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58020a9-e21c-496c-9c9d-aa1b2362b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing stage\n",
    "# Saving columns needed for further Elo calculation\n",
    "cols_to_save = [\n",
    "    \"match_id\",\n",
    "    \"match_week\",\n",
    "    \"home_team\",\n",
    "    \"away_team\",\n",
    "    \"home_score\",\n",
    "    \"away_score\",\n",
    "]\n",
    "bundesliga_matches_filt = bundesliga_matches[cols_to_save].reset_index(drop=True)\n",
    "# Feature engineering: creating a column 'match_result' - to know the outcome, HW - home win, AW - away win, D - draw\n",
    "conditions = [\n",
    "    bundesliga_matches_filt[\"home_score\"] > bundesliga_matches_filt[\"away_score\"],\n",
    "    bundesliga_matches_filt[\"home_score\"] < bundesliga_matches_filt[\"away_score\"],\n",
    "]\n",
    "choices = [\"HW\", \"AW\"]\n",
    "bundesliga_matches_filt[\"match_result\"] = np.select(conditions, choices, default=\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39223984-58aa-4507-9ac0-76bf0a1218ba",
   "metadata": {},
   "source": [
    "#### Performance Data Visaulisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca409e4-cd1a-4632-98cd-3aff06aa9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualisation stage focusing on teams' performance, that would give us the first assumptions about Elo rating\n",
    "\n",
    "# Creating the home and away data separately\n",
    "home_stats = bundesliga_matches_filt[\n",
    "    [\"home_team\", \"home_score\", \"away_score\", \"match_result\"]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"home_team\": \"team\",\n",
    "        \"home_score\": \"goals_scored\",\n",
    "        \"away_score\": \"goals_conceded\",\n",
    "    }\n",
    ")\n",
    "away_stats = bundesliga_matches_filt[\n",
    "    [\"away_team\", \"away_score\", \"home_score\", \"match_result\"]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"away_team\": \"team\",\n",
    "        \"away_score\": \"goals_scored\",\n",
    "        \"home_score\": \"goals_conceded\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setting match results for home and away\n",
    "home_stats[\"win\"] = home_stats[\"match_result\"] == \"HW\"\n",
    "home_stats[\"loss\"] = home_stats[\"match_result\"] == \"AW\"\n",
    "home_stats[\"draw\"] = home_stats[\"match_result\"] == \"D\"\n",
    "\n",
    "away_stats[\"win\"] = away_stats[\"match_result\"] == \"AW\"\n",
    "away_stats[\"loss\"] = away_stats[\"match_result\"] == \"HW\"\n",
    "away_stats[\"draw\"] = away_stats[\"match_result\"] == \"D\"\n",
    "\n",
    "# Concatenating home and away data\n",
    "all_stats = pd.concat([home_stats, away_stats])\n",
    "\n",
    "# Aggregating results for each team\n",
    "teams_performance_stats = (\n",
    "    all_stats.groupby(\"team\")\n",
    "    .agg(\n",
    "        goals_scored=(\"goals_scored\", \"sum\"),\n",
    "        goals_conceded=(\"goals_conceded\", \"sum\"),\n",
    "        wins=(\"win\", \"sum\"),\n",
    "        losses=(\"loss\", \"sum\"),\n",
    "        draws=(\"draw\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "teams_performance_stats[\"points\"] = (\n",
    "    teams_performance_stats[\"wins\"] * 3 + teams_performance_stats[\"draws\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279b593-72a4-430d-b722-d340ebc0525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for goals data visualisation\n",
    "sorted_teams_stats_goals = teams_performance_stats.sort_values(\n",
    "    by=\"goals_scored\", ascending=True\n",
    ")\n",
    "teams_sorted = sorted_teams_stats_goals[\"team\"]\n",
    "goals_scored_sorted = sorted_teams_stats_goals[\"goals_scored\"]\n",
    "goals_conceded_sorted = sorted_teams_stats_goals[\"goals_conceded\"]\n",
    "\n",
    "# Creating the horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(teams_sorted, goals_scored_sorted, color=\"blue\", label=\"Goals Scored\")\n",
    "plt.barh(teams_sorted, -goals_conceded_sorted, color=\"red\", label=\"Goals Conceded\")\n",
    "\n",
    "plt.xlabel(\"Number of Goals\")\n",
    "plt.title(\"Goals Scored vs. Goals Conceded by Team (Sorted by Goals Scored)\")\n",
    "plt.axvline(0, color=\"black\", linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d60f3-fd01-4e87-936f-378ad7891a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for match result data visualisation\n",
    "sorted_teams_stats_wins = teams_performance_stats.sort_values(by=\"wins\", ascending=True)\n",
    "teams_sorted_wins = sorted_teams_stats_wins[\"team\"]\n",
    "wins_sorted = sorted_teams_stats_wins[\"wins\"]\n",
    "losses_sorted = sorted_teams_stats_wins[\"losses\"]\n",
    "draws_sorted = sorted_teams_stats_wins[\"draws\"]\n",
    "\n",
    "# Plotting grouped bar\n",
    "bar_width = 0.2\n",
    "positions = range(len(teams_sorted_wins))\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.barh(\n",
    "    [p + bar_width for p in positions],\n",
    "    wins_sorted,\n",
    "    height=bar_width,\n",
    "    color=\"green\",\n",
    "    label=\"Wins\",\n",
    ")\n",
    "plt.barh(positions, losses_sorted, height=bar_width, color=\"red\", label=\"Losses\")\n",
    "plt.barh(\n",
    "    [p - bar_width for p in positions],\n",
    "    draws_sorted,\n",
    "    height=bar_width,\n",
    "    color=\"gray\",\n",
    "    label=\"Draws\",\n",
    ")\n",
    "\n",
    "plt.yticks(positions, teams_sorted_wins)\n",
    "plt.xlabel(\"Number of Matches\")\n",
    "plt.title(\"Wins, Losses, and Draws per Team (Sorted by Wins)\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d745c89-823c-4ee8-83c8-64c8078dd665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for points gained data visualisation\n",
    "sorted_teams_stats_points = teams_performance_stats.sort_values(\n",
    "    by=\"points\", ascending=True\n",
    ")\n",
    "\n",
    "# Plotting the horizontal bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(\n",
    "    sorted_teams_stats_points[\"team\"],\n",
    "    sorted_teams_stats_points[\"points\"],\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "\n",
    "# Adding text annotations beside each bar\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width() + 2,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{int(bar.get_width())}\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        color=\"gray\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "plt.xlabel(\"Points\")\n",
    "plt.title(\"Points Gained by Teams (Highest to Least)\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940a510-eccc-4402-9d64-7552a6ca02b9",
   "metadata": {},
   "source": [
    "The main performance indicators, such as goals ratio, match results, and total points gained, reveal several common trends:\n",
    "\n",
    "1. **Top Performers**  \n",
    "   Two main teams, **Bayern Munich** and **Borussia Dortmund**, significantly outperformed all others. Although both are evidently the strongest, there is a discrepancy in their power, with Bayern Munich holding a lead.\n",
    "\n",
    "2. **Second Tier Teams**  \n",
    "   The next level of teams, following the top two, includes **Bayer Leverkusen**, **Borussia Mönchengladbach**, and **Schalke 04**.\n",
    "\n",
    "3. **Weakest Team**  \n",
    "   The weakest team in the 2015/2016 Bundesliga season was **Hannover 96**, by a considerable margin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0588366-8b30-4015-ae30-1ff615b4c4a4",
   "metadata": {},
   "source": [
    "### Calculate Elo Rating with default hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab7a36-90b1-43a3-bbd2-8c2f42a88c4f",
   "metadata": {},
   "source": [
    "The goal of this block is to create an Elo calculation pipeline, tested with default values: `initial_elo`, `s_factor`, and `k_factor`. This pipeline will be used in the next block for Elo experimentation, fine-tuning the `s_factor` and `k_factor` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f5fb8-efc7-454d-a3b0-d7046a72f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to intialise the input for Elo calculation\n",
    "def initialize_elo_ratings(\n",
    "    team_names: List[str], initial_elo: int = 100\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"Initialize Elo ratings for each team with the default initial Elo score.\n",
    "\n",
    "    Args:\n",
    "        team_names: Names of teams for which to create initial Elo.\n",
    "        initial_elo (optional): Value of initial Elo, default is 100.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary where keys are team's names, values are initial Elo.\n",
    "\n",
    "    \"\"\"\n",
    "    return {team_name: initial_elo for team_name in team_names}\n",
    "\n",
    "\n",
    "def initialize_teams_elo_stats(team_names: List[str]) -> DataFrame:\n",
    "    \"\"\"Initialize the teams_stats dataframe with empty lists for Elo rating tracking.\n",
    "\n",
    "    Args:\n",
    "        team_names: Names of teams to use as index for a created DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe with index as team names, and three columns, all of List types,\n",
    "        to store the history of important measurements during Elo calculation.\n",
    "\n",
    "    \"\"\"\n",
    "    teams_stats = pd.DataFrame(\n",
    "        index=team_names,\n",
    "        columns=[\n",
    "            \"expected_win_prob_history\",\n",
    "            \"actual_results_history\",\n",
    "            \"elo_rating_history\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    for col in teams_stats.columns:\n",
    "        teams_stats[col] = [[] for _ in range(len(teams_stats))]\n",
    "        \n",
    "    return teams_stats\n",
    "\n",
    "\n",
    "def initialize_input_for_elo_calc(\n",
    "    team_names: List[str], initial_elo: int = 100\n",
    ") -> Tuple[DataFrame, Dict[str, int]]:\n",
    "    \"\"\"Initialize two inputs for further Elo calculation.\n",
    "\n",
    "    Args:\n",
    "        team_names: Names of teams to use as index for a created DataFrame.\n",
    "        initial_elo: Value of initial Elo, default is 100.\n",
    "\n",
    "    Returns:\n",
    "        _description_\n",
    "\n",
    "    \"\"\"\n",
    "    teams_elo_stats = initialize_teams_elo_stats(team_names)\n",
    "    elo_ratings = initialize_elo_ratings(team_names, initial_elo)\n",
    "\n",
    "    return teams_elo_stats, elo_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1ff52-b3f9-4046-88ff-dbfdc13da82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_win_prob(\n",
    "    elo_rating_diff: Union[int, float], s_factor: int = 15, base: int = 10\n",
    ") -> float:\n",
    "    \"\"\"Calculate the expected win probability based on Elo ratings difference.\n",
    "\n",
    "    Args:\n",
    "        rating_diff: A difference between Elo ratings of home and away teams.\n",
    "        s_factor (optional): A scaling factor of Elo rating. Default is 15.\n",
    "        base (optional): The base value used in the power calculation, defaults to 10\n",
    "            Although it is uncommon to change this parameter in Elo calculations, it is added for more flexibility.\n",
    "\n",
    "    Returns:\n",
    "        The expected probability of a home team win.\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 / (1 + base ** (-elo_rating_diff / s_factor))\n",
    "\n",
    "\n",
    "def get_actual_result(match_result: str) -> Tuple[float, float]:\n",
    "    \"\"\"Convert string match result to actual results values for home and away teams for further Elo calculation.\n",
    "    1 - for wining, 0 - for loosing, 0.5 - for a draw.\n",
    "\n",
    "    Args:\n",
    "        match_result: Indicates the outcome of a match.\n",
    "\n",
    "    Returns:\n",
    "        Number-values of actual match results for home and away teams.\n",
    "\n",
    "    \"\"\"\n",
    "    if match_result == \"HW\":\n",
    "        return 1.0, 0.0\n",
    "    elif match_result == \"AW\":\n",
    "        return 0.0, 1.0\n",
    "    elif match_result == \"D\":\n",
    "        return 0.5, 0.5\n",
    "\n",
    "\n",
    "def update_elo_rating(\n",
    "    current_rating: Union[float, int],\n",
    "    actual_result: float,\n",
    "    expected_win_prob: float,\n",
    "    k_factor: int = 15,\n",
    ") -> Union[float, int]:\n",
    "    \"\"\"Calculate an updated Elo rating based on actual and expected results.\n",
    "\n",
    "    Args:\n",
    "        current_rating: Elo rating to  be updated based on difference between actual and predicted results.\n",
    "        actual_result: An actual result for a team, could be aither 1, 0.5, 0 (see get_actual_result function).\n",
    "        expected_win_prob: A predicted win probability for a team (see calculate_expected_win_prob function)\n",
    "        k_factor (optional): A factor influencing how much a difference between actual and predicted results\n",
    "            will change team's current Elo rating, default is 15.\n",
    "\n",
    "    Returns:\n",
    "        Updated team's Elo rating.\n",
    "\n",
    "    \"\"\"\n",
    "    # max - to avoid possible cases when Elo goes negative \n",
    "    updated_elo_rating = max(\n",
    "        round(current_rating + k_factor * (actual_result - expected_win_prob), 4), 0\n",
    "    )\n",
    "\n",
    "    return updated_elo_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7d216-700c-4116-8c48-640f663914ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_match_week(\n",
    "    matches_by_week: DataFrame,\n",
    "    current_elo_ratings: Dict[str, Union[float, int]],\n",
    "    s_factor: int = 15,\n",
    "    k_factor: int = 15,\n",
    ") -> Dict[str, List[Union[float, int]]]:\n",
    "    \"\"\"Process each match in a given match week, calculating and storing updated Elo ratings,\n",
    "    also saving the history of sub-measurements like expected win probabilities and actual results.\n",
    "\n",
    "    Args:\n",
    "        matches_by_week: Dataframe with all matches happened in the same macth week.\n",
    "        current_elo_ratings: Dictionary where keys are teams' names, and values\n",
    "            their respective Elo ratings of a previous match week that will be updated based on results of this week\n",
    "        s_factor (optional): Scaling factor of Elo rating, default is 15.\n",
    "        k_factor (optional): A factor influencing how much the difference between actual and predicted results\n",
    "            will change team's current Elo rating, default is 15.\n",
    "    Returns:\n",
    "        A dictionary with keys as teams' names and values as Elo-related measurements of this match week,\n",
    "        particularly expected win probs, actual results, and changed Elo ratings.\n",
    "\n",
    "    \"\"\"\n",
    "    week_updates = {}\n",
    "\n",
    "    for _, match in matches_by_week.iterrows():\n",
    "        home_team, away_team, result = (\n",
    "            match[\"home_team\"],\n",
    "            match[\"away_team\"],\n",
    "            match[\"match_result\"],\n",
    "        )\n",
    "        current_home_elo, current_away_elo = (\n",
    "            current_elo_ratings[home_team],\n",
    "            current_elo_ratings[away_team],\n",
    "        )\n",
    "\n",
    "        # Calculate expected win probabilities\n",
    "        expected_home = calculate_expected_win_prob(\n",
    "            current_home_elo - current_away_elo, s_factor\n",
    "        )\n",
    "        expected_away = 1 - expected_home\n",
    "\n",
    "        # Calculate actual results\n",
    "        actual_home, actual_away = get_actual_result(result)\n",
    "\n",
    "        # Update Elo ratings\n",
    "        updated_home_elo = update_elo_rating(\n",
    "            current_home_elo, actual_home, expected_home, k_factor\n",
    "        )\n",
    "        updated_away_elo = update_elo_rating(\n",
    "            current_away_elo, actual_away, expected_away, k_factor\n",
    "        )\n",
    "\n",
    "        # Store results in the temporary dictionary\n",
    "        week_updates[home_team] = [expected_home, actual_home, updated_home_elo]\n",
    "        week_updates[away_team] = [expected_away, actual_away, updated_away_elo]\n",
    "\n",
    "    return week_updates\n",
    "\n",
    "\n",
    "def update_teams_stats(\n",
    "    teams_elo_stats: DataFrame, week_updates: Dict[str, List[Union[float, int]]]\n",
    ") -> DataFrame:\n",
    "    \"\"\"Update history of Elo-related columns in teams_elo_stats dataframe with weekly updates.\n",
    "\n",
    "    Args:\n",
    "        teams_elo_stats: DataFrame with indexes as teams' names and with columns of type list\n",
    "            which stores weekly updates of Elo-related meauserements\n",
    "        week_updates: A dictionary with Elo-related meauserements' updates (see process_match_week function)\n",
    "    Returns:\n",
    "        A df with updated history Elo-related indicators\n",
    "\n",
    "    \"\"\"\n",
    "    for team, updates in week_updates.items():\n",
    "        for i, value in enumerate(updates):\n",
    "            teams_elo_stats.loc[team].iat[i].append(value)\n",
    "\n",
    "    return teams_elo_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7ae17-0160-42d6-a411-daf742f1e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_elo_ratings(\n",
    "    preprocessed_matches_df: DataFrame,\n",
    "    teams_elo_stats_init: DataFrame,\n",
    "    elo_ratings_init: Dict[str, int],\n",
    "    s_factor: int = 15,\n",
    "    k_factor: int = 15,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Main function to calculate Elo ratings based on match data.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_matches_df: Preprocessed match data (see Preprocessing part).\n",
    "        teams_elo_stats_init: An empty df with predifiened struture to store Elo calculation results (see initialize_teams_elo_stats function)\n",
    "        elo_ratings_init: Dictionary where keys are teams' names, values are initial Elo (see initialize_elo_ratings function).\n",
    "        s_factor (optional): Scaling factor of Elo rating, default is 15.\n",
    "        k_factor (optional): A factor influencing how much the difference between actual and predicted results\n",
    "            will change team's current Elo rating, default is 15.\n",
    "    Returns:\n",
    "        A df with index as teams' names, with the following columns:\n",
    "        'expected_win_prob_history', 'actual_results_history', 'elo_rating_history', 'final_elo_rating'.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty teams_elo_stats\n",
    "    teams_elo_stats = teams_elo_stats_init.applymap(\n",
    "        lambda x: copy.deepcopy(x) if isinstance(x, list) else x\n",
    "    )\n",
    "    # start with initial Elo ratings, after each week current_elo_ratings changes\n",
    "    current_elo_ratings = elo_ratings_init.copy()\n",
    "\n",
    "    # Process each match week and consequently update teams_elo_stats\n",
    "    for week, matches in preprocessed_matches_df.groupby(\"match_week\"):\n",
    "        week_updates = process_match_week(\n",
    "            matches, current_elo_ratings, s_factor, k_factor\n",
    "        )\n",
    "        update_teams_stats(teams_elo_stats, week_updates)\n",
    "        # updated Elo ratings are third elements in week_updates\n",
    "        current_elo_ratings = {k: v[2] for k, v in week_updates.items()}\n",
    "    # final_elo_rating - Elo rating based on the whole season\n",
    "    teams_elo_stats[\"final_elo_rating\"] = teams_elo_stats[\"elo_rating_history\"].apply(\n",
    "        lambda x: x[-1]\n",
    "    )\n",
    "    teams_elo_stats.sort_values(by=\"final_elo_rating\", ascending=False, inplace=True)\n",
    "\n",
    "    return teams_elo_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20edec-411d-4288-9a28-d5d977f648ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the Elo rating pipeline on whole season with default values\n",
    "teams = teams_performance_stats[\"team\"].unique()\n",
    "teams_elo_stats_init, elo_ratings_init = initialize_input_for_elo_calc(teams)\n",
    "teams_elo_stats = calculate_elo_ratings(\n",
    "    bundesliga_matches_filt, teams_elo_stats_init, elo_ratings_init\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f87060-b6fa-4f09-a787-2db88bba978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Iterate through each team and plot their Elo rating history\n",
    "for team in teams_elo_stats.index:\n",
    "    elo_history = teams_elo_stats.loc[team, \"elo_rating_history\"]\n",
    "    plt.plot(range(len(elo_history)), elo_history, marker=\"o\", label=team)\n",
    "\n",
    "plt.xlabel(\"Match Week\")\n",
    "plt.ylabel(\"Elo Rating\")\n",
    "plt.title(\"Elo Rating History of Teams\")\n",
    "plt.legend(title=\"Teams\", loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a70c0b-7066-47cf-8268-a97164e83a4a",
   "metadata": {},
   "source": [
    "The pipeline with default hyperparameters was tested successfully. However, the visualization of Elo ratings for Bundesliga teams shows that the ranking is chaotic and does not reflect the trends detected during the performance analysis. **Therefore, a better set of hyperparameters needs to be found, which will be addressed in the next block**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb17df-5e7c-46a5-bbf9-8cda89190325",
   "metadata": {},
   "source": [
    "### Find the best s_factor and k_factor for Elo calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5153e-336c-463f-b6de-23a1010a28d1",
   "metadata": {},
   "source": [
    "The next step in developing the Elo rating system is to identify the best combination of `s_factor` and `k_factor`. The approach involves calculating the Mean Absolute Error (MAE) between the expected win probabilities and actual match results for each combination of `s_factor` and `k_factor` across the entire season. To evaluate the performance of each hyperparameter combination, the following visualizations will be used:\n",
    "\n",
    "1. **Line Graph of Teams' Elo Ratings Evolution Per Week**  \n",
    "   This graph shows how the Elo ratings of each team change throughout the season.\n",
    "\n",
    "2. **Line Graph of Teams' MAE Losses Evolution Per Week**  \n",
    "   This visualization tracks the MAE losses for each team over the course of the season.\n",
    "\n",
    "3. **Bar Plot of Mean Seasonal MAE Losses**  \n",
    "   This plot provides a summary of the average MAE loss for each combination across a whole season season.\n",
    "\n",
    "4. **Bar Plot of Accuracy between Elo Rankings and Actual Rankings (Based on Points Gained)**  \n",
    "   This comparison assesses how closely the Elo rankings align with the actual team rankings based on points earned.\n",
    "\n",
    "The most straightforward method for selecting the optimal combination is to choose the `s_factor` and `k_factor` pair with the lowest seasonal MAE loss and highest accuracy. However, additional visualizations will also be considered to guide the final decision.\n",
    "\n",
    "*Note:* Initially, Mean Squared Error (MSE) was included as a loss metric to be tracked and visualized. However, experimental results indicated that MSE did not offer any additional insights beyond what was already provided by Mean Absolute Error (MAE). As a result, MSE was excluded from the evaluation to enhance the readability of the graphs and keep the visualizations focused and concise, showing only the most relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e1409-294a-44bc-b25e-f190bedc74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_loss_history(teams_elo_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\"Updates teams_elo_stats dataframe adding information about MAE loss for each match.\n",
    "\n",
    "    Args:\n",
    "        teams_elo_stats: A dataframe with Elo-related indicators ('expected_win_prob_history' and 'actual_results_history')\n",
    "\n",
    "    Returns:\n",
    "        An updated dataframe with a new column 'mae_loss_history'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    teams_elo_stats[\"mae_loss_history\"] = [[] for _ in range(len(teams_elo_stats))]\n",
    "\n",
    "    for team in teams_elo_stats.index:\n",
    "        expected = teams_elo_stats.loc[team, \"expected_win_prob_history\"]\n",
    "        actual = teams_elo_stats.loc[team, \"actual_results_history\"]\n",
    "        mae_losses = []\n",
    "        for a, e in zip(actual, expected):\n",
    "            mae = mean_absolute_error([a], [e])\n",
    "            mae_losses.append(mae)\n",
    "        teams_elo_stats.at[team, \"mae_loss_history\"] = mae_losses\n",
    "\n",
    "    return teams_elo_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d68025-af03-422d-8c97-bbb5e71ffc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot evaluation metrics\n",
    "def create_metrics_line_graph(\n",
    "    df: DataFrame,\n",
    "    column: str,\n",
    "    y_label: str,\n",
    "    title: str,\n",
    "    ax: Axes,\n",
    ") -> None:\n",
    "    \"\"\"Helper function to ceate a line graph for metrics of different teams over time.\n",
    "\n",
    "    This function plots a line graph for each team based on historical data from a specified column in the DataFrame.\n",
    "    Each line represents a team's Elo-related metric over match weeks, displayed on a specified subplot.\n",
    "\n",
    "    Args:\n",
    "        df: A dataframe containing the historical metrics data for multiple teams, with teams as index\n",
    "            and a column of lists representing metrics history.\n",
    "        column: The name of the column in the DataFrame containing lists of historical data points for each team.\n",
    "        y_label: The label for the Y-axis, representing the metric being visualized.\n",
    "        title: The title of the graph, usually describing the metric being plotted.\n",
    "        ax: Axes object where the graph will be plotted.\n",
    "\n",
    "    Returns:\n",
    "        This function does not return anything. It directly modifies the plot on the given Axes object.\n",
    "\n",
    "    \"\"\"\n",
    "    for team in df.index:\n",
    "        column_history = df.loc[team, column]\n",
    "        ax.plot(range(len(column_history)), column_history, marker=\"o\", label=team)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel(\"Match Week\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(title=\"Teams\", loc=\"upper left\", fontsize=\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9d8be-0a74-4dc6-95da-50a74eec9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weekly_evaluation_metrics(\n",
    "    teams_stats: DataFrame, s_factor: float, k_factor: float\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Create a combined evaluation plot with Elo rating and MAE loss.\n",
    "\n",
    "    This function generates a combined plot that includes line graphs of Elo ratings and MAE losses for all teams over time.\n",
    "    Helps in assessing the performance and consistency of Elo calculation with different s_factor and k_factor\n",
    "\n",
    "    Args:\n",
    "        teams_stats: A dataframe containing historical data for teams, including Elo rating and MAE loss histories.\n",
    "        s_factor: The scaling factor used in Elo calculation, shown in the plot title.\n",
    "        k_factor: The adjustment factor for rating changes, shown in the plot title.\n",
    "\n",
    "    Returns:\n",
    "        This function does not return anything; it directly creates the plot\n",
    "\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    create_metrics_line_graph(\n",
    "        teams_stats,\n",
    "        \"elo_rating_history\",\n",
    "        \"Elo Rating\",\n",
    "        \"Elo Rating History of Teams\",\n",
    "        axes[0],\n",
    "    )\n",
    "    create_metrics_line_graph(\n",
    "        teams_stats,\n",
    "        \"mae_loss_history\",\n",
    "        \"MAE Loss\",\n",
    "        \"MAE Loss History of Teams\",\n",
    "        axes[1],\n",
    "    )\n",
    "\n",
    "    # Set main title reflecting current scale and k factors\n",
    "    fig.suptitle(\n",
    "        f\"Evaluation Metrics for s_factor={s_factor:.2f} and k_factor={k_factor:.2f}\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d084b9-6c4d-4200-91c2-5a197846fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seasonal_evaluational_metrics(\n",
    "    teams_elo_stats: pd.DataFrame, actual_teams_rankings: List[str]\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"Calculates seasonal evaluation metrics:\n",
    "\n",
    "    1. Seasonal Mean MAE: The average of all Mean Absolute Error (MAE) values recorded throughout the season.\n",
    "    2. Seasonal Accuracy: The number of teams whose Elo ranking matches their actual ranking, reflecting how accurately\n",
    "       the Elo system represents the true standings.\n",
    "\n",
    "    Args:\n",
    "        teams_elo_stats: DataFrame containing Elo-related statistics for each team\n",
    "        actual_teams_rankings: A list of final actual teams' rankings for the season based on the points gained\n",
    "\n",
    "    Returns:\n",
    "        seasonal_mean_mae (float): The rounded average MAE for the entire season.\n",
    "        seasonal_accuracy (int): The count of matching positions between Elo rankings and actual team rankings.\n",
    "\n",
    "    \"\"\"\n",
    "    all_mae_values = [\n",
    "        value for sublist in teams_elo_stats[\"mae_loss_history\"] for value in sublist\n",
    "    ]\n",
    "    seasonal_mean_mae = round(sum(all_mae_values) / len(all_mae_values), 4)\n",
    "    # in previous steps elo_teams_rankings was sorted by final_elo_rating in descending order\n",
    "    elo_teams_rankings = list(teams_elo_stats.index)\n",
    "    seasonal_accuracy = sum(\n",
    "        1\n",
    "        for elo, actual in zip(elo_teams_rankings, actual_teams_rankings)\n",
    "        if elo == actual\n",
    "    )\n",
    "\n",
    "    return seasonal_mean_mae, seasonal_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9787bc9-c529-4396-b0ac-884562ab8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_bar_graph(\n",
    "    data: List[float],\n",
    "    keys: List[Tuple[float, float]],\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    ax: Axes,\n",
    ") -> None:\n",
    "    \"\"\"Helper function to plot a simple bar graph.\n",
    "\n",
    "    Args:\n",
    "        data: A list of values to be plotted as bars.\n",
    "        keys: A list of (s_factor, k_factor) tuples for the x-axis labels.\n",
    "        title: The title of the plot.\n",
    "        ylabel: The label for the y-axis.\n",
    "        ax: The axes object where the plot will be drawn.\n",
    "\n",
    "    Returns:\n",
    "        This function does not return anything; it directly creates a plot.\n",
    "\n",
    "    \"\"\"\n",
    "    x = range(len(data))\n",
    "    ax.bar(x, data, color=\"skyblue\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"(s_factor, k_factor)\", fontsize=12)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"{k}\" for k in keys], rotation=45)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "def plot_seasonal_evaluation_history(\n",
    "    results: Dict[Tuple[float, float], List[float]]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Two bar charts of mean MAE loss of a whole season and Elo rankings accuracy\n",
    "    for different (s_factor, k_factor) configurations.\n",
    "\n",
    "    Args:\n",
    "        results: A dictionary where keys are tuples representing (s_factor, k_factor) configurations,\n",
    "            and values are lists containing their respective seasonal_mean_mae, seasonal_accuracy values.\n",
    "\n",
    "    Returns:\n",
    "        This function does not return anything; it directly creates a plot.\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract keys and values from the dictionary\n",
    "    keys = list(results.keys())\n",
    "    seasonal_mean_maes = [v[0] for v in results.values()]\n",
    "    seasonal_accuracies = [v[1] for v in results.values()]\n",
    "\n",
    "    # Create figure with two subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    # Plot MSE and MAE seasonal means\n",
    "    create_metrics_bar_graph(\n",
    "        seasonal_mean_maes, keys, \"Mean MAE of the season\", \"Mean MAE\", axes[0]\n",
    "    )\n",
    "    create_metrics_bar_graph(\n",
    "        seasonal_accuracies, keys, \"Accuracy of Elo teams' ranking\", \"Accuracy\", axes[1]\n",
    "    )\n",
    "\n",
    "    fig.suptitle(f\"Seasonal Evaluation Metrics for different hyperparams\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8d86b-af14-4c17-a774-e11a09afe6e0",
   "metadata": {},
   "source": [
    "To start finding the best combination, it is useful to understand the approximate ranges of values and the relationships between `s_factor` and `k_factor` that are best suited for the Elo rating. Based on existing literature on this topic, we can assume some potential good combinations.\n",
    "\n",
    "Below are manually defined combinations aiming to test some 'general cases':\n",
    "\n",
    "1. **`s_factor = 15`, `k_factor = 15`**  \n",
    "   Both hyperparameters are equal and small.\n",
    "\n",
    "2. **`s_factor = 400`, `k_factor = 400`**  \n",
    "   Both hyperparameters are equal and large.\n",
    "\n",
    "3. **`s_factor = 15`, `k_factor = 400`**  \n",
    "   `s_factor` is small, while `k_factor` is large.\n",
    "\n",
    "4. **`s_factor = 400`, `k_factor = 15`**  \n",
    "   `s_factor` is large, while `k_factor` is small.\n",
    "\n",
    "Let's test these combinations and see which one performs the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2e3ca-6d80-4bf9-9134-7c8af9809a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_evaluation_history = {}\n",
    "actual_teams_rankings = sorted_teams_stats_points[\"team\"].to_list()[::-1]\n",
    "\n",
    "combinations = [(15, 15), (400, 400), (15, 400), (400, 15)]\n",
    "\n",
    "for combination in combinations:\n",
    "\n",
    "    s_factor, k_factor = combination\n",
    "\n",
    "    teams_elo_stats = calculate_elo_ratings(\n",
    "        bundesliga_matches_filt,\n",
    "        teams_elo_stats_init,\n",
    "        elo_ratings_init,\n",
    "        s_factor,\n",
    "        k_factor,\n",
    "    )\n",
    "\n",
    "    teams_elo_stats = update_loss_history(teams_elo_stats)\n",
    "\n",
    "    plot_weekly_evaluation_metrics(teams_elo_stats, s_factor, k_factor)\n",
    "\n",
    "    seasonal_evaluation_history[combination] = calculate_seasonal_evaluational_metrics(\n",
    "        teams_elo_stats, actual_teams_rankings\n",
    "    )\n",
    "\n",
    "plot_seasonal_evaluation_history(seasonal_evaluation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463beb05-1eb2-40fa-acc5-9c363ca85add",
   "metadata": {},
   "source": [
    "The results clearly show the advantage and strength of the combination `s_factor = 400` and `k_factor = 15`.\n",
    "\n",
    "- **`s_factor = 15` & `k_factor = 15` and `s_factor = 400` & `k_factor = 400`**  \n",
    "  These combinations show almost similar performance. The Elo ratings evolution produced by these settings is chaotic and does not improve over time, contradicting the performance trends identified earlier. The losses are high and final predictions are not accurate.\n",
    "\n",
    "- **`s_factor = 15` & `k_factor = 400`**  \n",
    "  This is by far the worst combination (please do not be misled by accuracy, it is very small). The Elo system evolution is total chaos, failing to show any smoothed trends. The losses are extremely high, with a substantial portion being equal to 1, indicating that the predicted results are completely incorrect.\n",
    "\n",
    "- **`s_factor = 400` & `k_factor = 15`**  \n",
    "  This combination is a clear winner. It is the first configuration that, at a high level, reflects real performance results while having the lowest loss rates and good accuracy.\n",
    "\n",
    "Based on existing literature ([Elo Rating System](https://en.wikipedia.org/wiki/Elo_rating_system), [Rpubs Analysis](https://rpubs.com/DTS098/SPE5AMS_Portfolio02), etc.), the success of the final configuration is understandable, as these values are quite common in general in Elo calculations. The scale factor and the K-factor serve distinct purposes in the Elo rating system, and their values are set differently to balance the system's responsiveness and stability:\n",
    "\n",
    "- A higher scale factor ensures that expected scores are not overly sensitive to rating differences.\n",
    "- The K-factor is deliberately kept lower to moderate the impact of individual match results. If it was higher than the scale factor, ratings would change too drastically based on each game, leading to an unstable rating environment.\n",
    "\n",
    "Thus, the relationship and approximate ranges of `s_factor` and `k_factor` are generally well understood. While `s_factor = 400` and `k_factor = 15` performed very well, exploring other combinations following the consistent logic described above may yield even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc1a52-a732-4a71-9b5d-4f4ac2c73c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_half_decade(values: ndarray) -> ndarray:\n",
    "    \"\"\"Rounds the given array of values to the nearest half-decade (e.g., 5, 10, 15).\n",
    "\n",
    "    This function divides each value by 5, rounds it to the nearest integer,\n",
    "    and then multiplies it back by 5 to achieve rounding to the nearest multiple of 5.\n",
    "\n",
    "    Args:\n",
    "        values: An array of numerical values to be rounded.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of values rounded to the nearest half-decade.\n",
    "\n",
    "    Example:\n",
    "        >>> values = np.array([12, 23, 37, 49])\n",
    "        >>> round_to_nearest_half_decade(values)\n",
    "        array([10., 25., 35., 50.])\n",
    "\n",
    "    \"\"\"\n",
    "    return np.round(values / 5) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bac15b-2307-4ffd-b4ff-886325ccaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(\n",
    "    s_factor_range: List[int] = [100, 800],\n",
    "    k_factor_range: List[int] = [10, 50],\n",
    "    n: int = 10,\n",
    "    seed: int = 42,\n",
    ") -> List[int]:\n",
    "    \"\"\"Generate structured combinations of scale factors and K-factors.\n",
    "\n",
    "    This function generates linearly spaced values for both scale factors (s_factors)\n",
    "    and K-factors (k_factors) within the specified ranges. The s_factors are rounded\n",
    "    to the nearest decade, while the k_factors are rounded to the nearest half-decade.\n",
    "    Random combinations of these factors are then sampled to create a list of paired values.\n",
    "\n",
    "    Args:\n",
    "        s_factor_range: The range for the scale factors, specified as a list of two integers [min, max]. Defaults to [100, 800].\n",
    "        k_factor_range: The range for the K-factors, specified as a list of two integers [min, max]. Defaults to [10, 50].\n",
    "        n (optional): The number of values to generate for each factor. Defaults to 10.\n",
    "        seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, each containing a combination of rounded scale factor and K-factor values.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set the seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Create linear spaced values for more meaningful combinations\n",
    "    # s_factors are rounded to nearest decade\n",
    "    # k_factors are rounded to nearest half decade\n",
    "    s_factors = np.round(np.linspace(s_factor_range[0], s_factor_range[1], n), -1)\n",
    "    k_factors = round_to_nearest_half_decade(\n",
    "        np.linspace(k_factor_range[0], k_factor_range[1], n)\n",
    "    )\n",
    "\n",
    "    # Randomly sample from the created arrays\n",
    "    random_indices_s = np.random.choice(range(n), size=n, replace=False)\n",
    "    random_indices_k = np.random.choice(range(n), size=n, replace=False)\n",
    "    combinations = [\n",
    "        (round(s_factors[i_s]), round(k_factors[i_k]))\n",
    "        for i_s, i_k in zip(random_indices_s, random_indices_k)\n",
    "    ]\n",
    "\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c2876-fb77-475b-afea-230a995a0c20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seasonal_evaluation_history = {}\n",
    "# s_factor_range should be high since we do not want to make the model sensitive to a small rating differences\n",
    "# k_factor will be more flexible to find, we need to figure out what is a better way of k_factor to change: more drastically or more steadily\n",
    "combinations = generate_combinations(s_factor_range=[300, 600], n=10)\n",
    "\n",
    "for combination in combinations:\n",
    "\n",
    "    s_factor, k_factor = combination\n",
    "\n",
    "    teams_elo_stats = calculate_elo_ratings(\n",
    "        bundesliga_matches_filt,\n",
    "        teams_elo_stats_init,\n",
    "        elo_ratings_init,\n",
    "        s_factor,\n",
    "        k_factor,\n",
    "    )\n",
    "\n",
    "    teams_elo_stats = update_loss_history(teams_elo_stats)\n",
    "\n",
    "    plot_weekly_evaluation_metrics(teams_elo_stats, s_factor, k_factor)\n",
    "\n",
    "    seasonal_evaluation_history[combination] = calculate_seasonal_evaluational_metrics(\n",
    "        teams_elo_stats, actual_teams_rankings\n",
    "    )\n",
    "\n",
    "plot_seasonal_evaluation_history(seasonal_evaluation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9646be2-0a72-48a1-95ad-1ece2c3ffb08",
   "metadata": {},
   "source": [
    "All tested combinations **successfully captured the overall trends of team performances**, accurately reflecting the top five teams—Bayern Munich, Borussia Dortmund, Bayer Leverkusen, Borussia Mönchengladbach, and Schalke 04. The majority of the remaining teams were positioned in the middle, with frequent mixing, and two clear outliers, Stuttgart and Hannover 96, which aligns with the actual results of the season.\n",
    "\n",
    "Interestingly, **the different models showed minimal variation in MAE loss**. Despite this, a noticeable pattern emerged: the Elo rating system performed exceptionally well in predicting wins for some matches (with losses nearly at 0) but exhibited a higher error margin for others (with MAE fluctuating around the 0.5 mark). \n",
    "\n",
    "However, some combinations stood out for their accuracy and for producing Elo rating differences that were more distinct and closer to reality. These combinations consistently featured a smaller `k_factor` (with keeping `s_factor` high), suggesting that, **with this dataset, the Elo algorithm performs best when rating adjustments occur more steadily and gradually.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc61ccf-4e45-44bb-a9ce-13497e6e8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_factor_best, k_factor_best = 570, 10\n",
    "teams_elo_stats_best = calculate_elo_ratings(\n",
    "    bundesliga_matches_filt,\n",
    "    teams_elo_stats_init,\n",
    "    elo_ratings_init,\n",
    "    s_factor_best,\n",
    "    k_factor_best,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_elo_stats_best['points_gained_history'] = teams_elo_stats_best['actual_results_history'].apply(lambda results: [3 if x == 1 else 1 if x == 0.5 else 0 for x in results])\n",
    "    \n",
    "teams_elo_stats_best['points_gained_history'] = teams_elo_stats_best['points_gained_history'].apply(lambda points: pd.Series(points).cumsum().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 15))\n",
    "create_metrics_line_graph(\n",
    "    teams_elo_stats_best, \n",
    "    'points_gained_history', \n",
    "    'Points gained',\n",
    "    'Actuals points progression by match weeks', \n",
    "    axes[0]\n",
    ")\n",
    "create_metrics_line_graph(\n",
    "    teams_elo_stats_best, \n",
    "    'elo_rating_history',\n",
    "    'Elo rating with best hyperparameters', \n",
    "    'Elo ratings', \n",
    "    axes[1]\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649dd761-270a-4d73-9bcd-2bebfa81e8bc",
   "metadata": {},
   "source": [
    "### The single most surprising win"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e024e-2e52-40cc-9bde-b270b965859f",
   "metadata": {},
   "source": [
    "The most surprising win is defined as the one that produces the highest positive value of `actual_result - expected_win_prob`. Since the `k_factor` is stable throughout the Elo calculation, the most unexpected win will be the one that causes the largest increase in Elo rating compared to its previous value. This is determined using the Elo rating update formula: `new_rating = current_rating + k_factor * (actual_result - expected_win_prob)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4720c93-edaa-404b-b28c-77a6bfa1f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest_increase(df: DataFrame, column_name: str) -> Tuple[str, int]:\n",
    "    \"\"\"Find the team with the highest single-week increase in a specified column.\n",
    "\n",
    "    This function identifies the team that experienced the greatest increase in a specified metric\n",
    "    (e.g., Elo rating) from one match week to the next. It returns the team name and the match week\n",
    "    in which this highest increase occurred.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing historical data for multiple teams, with teams as index\n",
    "            and the specified metric as a column of lists.\n",
    "        column_name: The name of the column containing lists of historical metric values for each team.\n",
    "\n",
    "    Returns:\n",
    "        max_team: The name of the team with the highest increase.\n",
    "        match_week: The match week where the highest increase occurred.\n",
    "\n",
    "    \"\"\"\n",
    "    max_increase = -np.inf\n",
    "    max_team = None\n",
    "    max_index = -1\n",
    "\n",
    "    for team in df.index:\n",
    "        elo_history = df.loc[team, column_name]\n",
    "        for i in range(1, len(elo_history)):\n",
    "            increase = elo_history[i] - elo_history[i - 1]\n",
    "            if increase > max_increase:\n",
    "                max_increase = increase\n",
    "                max_team = team\n",
    "                max_index = i\n",
    "    # because in StatsBomb the first match week starts with 1 not with 0\n",
    "    match_week = max_index + 1\n",
    "\n",
    "    return max_team, match_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc67ee0-5793-447e-b204-0a8f336a2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_most_suprising_increase_params = find_highest_increase(\n",
    "    teams_elo_stats_best, \"elo_rating_history\"\n",
    ")\n",
    "# let's find this match in df\n",
    "# Properly formatted code for filtering matches\n",
    "suprising_match = bundesliga_matches[\n",
    "    (\n",
    "        (bundesliga_matches[\"home_team\"] == the_most_suprising_increase_params[0])\n",
    "        | (bundesliga_matches[\"away_team\"] == the_most_suprising_increase_params[0])\n",
    "    )\n",
    "    & (bundesliga_matches[\"match_week\"] == the_most_suprising_increase_params[1])\n",
    "]\n",
    "# This result is logical, as Eintracht Frankfurt was among the worst teams in the season,\n",
    "# while Borussia Dortmund was the second-best team in the 2015/2016 Bundesliga.\n",
    "# However, it's important to note that this was the second-to-last match of the season, with nothing at stake for Borussia Dortmund,\n",
    "# while Eintracht Frankfurt was fighting against relegation — a context that is challenging to capture in the Elo algorithm\n",
    "display(suprising_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49072f0-9598-41f3-b53f-1079939759a4",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46fffc7-e317-4971-9533-e1f0d576cb7c",
   "metadata": {},
   "source": [
    "The research developed **a flexible pipeline for Elo calculation**, enabling not only the computation of results but also providing a tool to fine-tune hyperparameters based on evaluation metrics. The pipeline was successfully tested on Bundesliga 2015/2016 data, with the final model accurately predicting the final positions of 13 out of 18 teams. **The model performs well for teams with relatively consistent match outcomes** (e.g., frequently winning or losing); however, it struggles with teams in the middle of the table that have fluctuating results, making them more challenging to predict accurately.\n",
    "\n",
    "One of **the most critical parameters** in achieving accurate Elo predictions was the **K-factor**, which determines the dynamics of rating changes. Smaller K-factor values tend to yield better results, especially given the limited and inconsistent match data, as gradual and steady rating adjustments are more suitable.\n",
    "\n",
    "While the final model performs well, there are several suggestions to improve the Elo rating algorithm further:\n",
    "\n",
    "1. **Use of Historical Data for Initial Elo Ratings**  \n",
    "   Incorporate previous seasons' data to set more advanced initial Elo ratings, improving the model's starting accuracy.\n",
    "\n",
    "2. **Dynamic K-factor Adjustment**  \n",
    "   Implement a dynamic K-factor that is relatively high at the beginning of the season—when teams are still finding their form—and decreases as the season progresses and teams' performances stabilize.\n",
    "\n",
    "3. **Incorporate Additional Match Context**  \n",
    "   Consider additional factors beyond the actual match result when updating Elo ratings, such as whether a team plays at home or away, or if the game has higher stakes (e.g., relegation battles). These factors could be integrated with simple additions or subtractions to the rating.\n",
    "\n",
    "4. **Comprehensive Training and Testing Process**  \n",
    "   Introduce a full training process using separate train and test datasets to evaluate how well the Elo calculation generalizes to unseen data. For example, the model could be trained on previous seasons to find the best hyperparameters and then used to predict the final results for an unseen season.\n",
    "\n",
    "These recommendations aim to improve Elo calculation, enhancing its predictive power and adaptability to various team performance patterns and match contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
